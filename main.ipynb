{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import scipy.io as sio\n",
    "\n",
    "# include exp and log functions\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Use this to return multivariable in one function\n",
    "import collections\n",
    "\n",
    "import time, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load matlab data.\n",
    "mat = sio.loadmat('data/spam.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_train = mat['train_spam']\n",
    "# train[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_test = mat['test_spam']\n",
    "len(_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weight(point, weight, alpha, f_t):\n",
    "    '''\n",
    "    This function takes in a observation point, its original weight and the current calculated alpha and\n",
    "    its weak learning function A. This function returns the new weight for this point.\n",
    "    This function should be called by using map/comprehension.\n",
    "    \n",
    "    point: a single column with length = feature_count + 1, the last one is the label\n",
    "    weight: should be a single value smaller than one, and normalized.\n",
    "    alpha: a value calculated with zt during every loop of adaboost.\n",
    "    f_t: a tuple with four elements (feature_index, feature_theta, left_node_class, right_node_class)\n",
    "    '''\n",
    "    \n",
    "    f_idx = f_t[0]\n",
    "    f_theta = f_t[1]\n",
    "    left_node_class = f_t[2]\n",
    "    right_node_class = f_t[3]\n",
    "        \n",
    "    label = point[-1]\n",
    "    if label == 0:\n",
    "        label = -1\n",
    "        \n",
    "    if point[f_idx] <= f_theta:\n",
    "        # classified to the left hand side node\n",
    "        classified_as = left_node_class\n",
    "    else:\n",
    "        classified_as = right_node_class\n",
    "        \n",
    "    new_weight = weight * math.exp((-1) * alpha * label * classified_as)\n",
    "    \n",
    "    return new_weight\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# try different alpha approach.\n",
    "# compare results with sklearn trained dt\n",
    "# last one can't be skipped... or not?\n",
    "\n",
    "def find_dt(train_data, weight):\n",
    "    '''\n",
    "    find_dt means find decision tree. For this function, we find a one level decision tree\n",
    "    \n",
    "    train_data: is the dataframe given into the function\n",
    "    weight: is an array the same length of the number of data points, and should be updated everytime before passing\n",
    "    into this function.\n",
    "    \n",
    "    returns feature index, theta value(threshold), left_node_class, right_node_class \n",
    "    '''\n",
    "    \n",
    "    feature_count = train_data.shape[1] - 1\n",
    "    \n",
    "    # loop through all features\n",
    "    costs = []\n",
    "    for idx in reversed(range(feature_count)):\n",
    "        features = train_data[:, idx]\n",
    "        labels = train_data[:, -1]\n",
    "        f_temp = np.vstack((features, labels, weight)).T\n",
    "        f_temp = f_temp[f_temp[:, 0].argsort(),:]\n",
    "        \n",
    "        # print(f_temp[:,0])\n",
    "        feature_costs = []\n",
    "        for idx, f_val in enumerate(f_temp[:,0]):\n",
    "            pass\n",
    "            \n",
    "            left_node_class, left_node_cost = 0, 1000\n",
    "            right_node_class, right_node_cost = 0, 1000\n",
    "        \n",
    "            if idx == len(f_temp) - 1:\n",
    "                continue\n",
    "            \n",
    "            if f_val == f_temp[idx+1 , 0]:\n",
    "                continue\n",
    "                \n",
    "            # if it's not the same as the next value, we split by its value.\n",
    "            # print(f_temp[0:idx+1,:])\n",
    "            # if left node is classified as zero\n",
    "            left_zero_cost = np.dot(f_temp[0:idx+1,1], f_temp[0:idx+1,2])\n",
    "            left_one_cost  = np.dot((f_temp[0:idx+1,1] -1)*(-1), f_temp[0:idx+1, 2])\n",
    "            right_zero_cost = np.dot(f_temp[idx+1:, 1], f_temp[idx+1:, 2])\n",
    "            right_one_cost  = np.dot( (f_temp[idx+1:, 1] -1)*(-1), f_temp[idx+1:, 2])\n",
    "    \n",
    "            if left_zero_cost <= left_one_cost:\n",
    "                left_node_class = -1\n",
    "                left_node_cost = left_zero_cost\n",
    "            else:\n",
    "                left_node_class = 1\n",
    "                left_node_cost = left_one_cost\n",
    "            \n",
    "            if right_zero_cost <= right_one_cost:\n",
    "                right_node_class = -1\n",
    "                right_node_cost = right_zero_cost\n",
    "            else:\n",
    "                right_node_class = 1\n",
    "                right_node_cost = right_one_cost\n",
    "            \n",
    "            cost = left_node_cost + right_node_cost\n",
    "            f_t = (cost, f_val, left_node_class, right_node_class)\n",
    "            feature_costs.append(f_t)\n",
    "        \n",
    "        # if all the points have the same value for this feature, we'll need this part\n",
    "        # that adds fake data into the list, so that the index won't be messed up when we want to find the\n",
    "        # feature that gives the smallest weighted cost.\n",
    "        if not feature_costs:\n",
    "            feature_costs.append((100, 0, 0,0))\n",
    "            \n",
    "        min_f = min(feature_costs, key=lambda x: x[0])\n",
    "        costs.append(min_f)\n",
    "    \n",
    "    # after looping through all the features, we look for the feature that gives us the smallest cost?\n",
    "    min_cost = min(costs, key=lambda x:x[0])\n",
    "    f_idx = costs[::-1].index(min_cost)\n",
    "\n",
    "    d_t = (f_idx,) + min_cost[1:]\n",
    "#     print('finding the best decision tree...')\n",
    "#     print(d_t)\n",
    "#     print('')\n",
    "    return d_t\n",
    "            \n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hw3_train_adaboost(train_data, num_rounds):\n",
    "    \n",
    "    # init variables to loop through\n",
    "    n = len(train_data)\n",
    "    features_count = len(train_data[0]) - 1 \n",
    "    \n",
    "    # init the weights for each data point\n",
    "    weights = [1/n] * n\n",
    "    \n",
    "    \n",
    "    # init the input vars to match my function's var names\n",
    "    T = num_rounds\n",
    "    train = train_data\n",
    "    \n",
    "    \n",
    "    # two list that saves all the alpha values and the weak function for each iteration\n",
    "    alphas = []\n",
    "    # this is a list that saves tuple with four element in each element\n",
    "    # (idx, theta, left_node_class, right_node_class)\n",
    "    # [(1,2,3,4), (1,2,3,4)...]\n",
    "    f_ts = []\n",
    "\n",
    "    \n",
    "    # make an idx list so we can reference back to the weights list\n",
    "    idx_list = [x for x in range(n)]\n",
    "    \n",
    "\n",
    "    # do T times\n",
    "    # loop through observations that finds the best 1 layer decision tree \n",
    "    for t in range(0, T):\n",
    "\n",
    "        # find the one level decision tree here\n",
    "        dt = find_dt(train, weights)\n",
    "        \n",
    "        feature_theta = dt[1]\n",
    "        feature_idx = dt[0]\n",
    "        left_node_class = dt[2]\n",
    "        right_node_class = dt[3]\n",
    "        \n",
    "        #######################################\n",
    "        # The Simple classifier is found here #\n",
    "        #######################################\n",
    "        \n",
    "#         print('========= DONE =========')\n",
    "#         print('f_idx: ' + str(feature_idx))\n",
    "#         print('f_theta: ' +  str(feature_theta) )\n",
    "#         print('left node class= ' + str(left_node_class))\n",
    "#         print('right node class= ' + str(right_node_class))\n",
    "#         print('')\n",
    "        \n",
    "        # update zt\n",
    "        zt = 0\n",
    "        for p_idx, point in enumerate(train[:, feature_idx]):\n",
    "            label = train[p_idx, -1]\n",
    "            # to calculate zt, we need to take 0 as -1, but keep the one.\n",
    "            if label == 0:\n",
    "                label = -1\n",
    "                \n",
    "            if point <= feature_theta:\n",
    "                # this will go to left node.\n",
    "                zt = zt + weights[p_idx] * (left_node_class * label)\n",
    "                # print('added ' + str(weights[p_idx] * (left_node_class * label)) )\n",
    "            else:\n",
    "                zt = zt + weights[p_idx] * (right_node_class * label)\n",
    "                # print('added ' + str(weights[p_idx] * (right_node_class * label)) )\n",
    "        \n",
    "        \n",
    "        # print('zt: ' + str(zt))\n",
    "                \n",
    "        # calculate alpha value for this loop\n",
    "        alpha = (1/2)* math.log((1 + zt)/(1 - zt))\n",
    "        # print('alpha: ' + str(alpha))\n",
    "        \n",
    "        alphas.append(alpha)\n",
    "\n",
    "        # append this function\n",
    "        f_t = (feature_idx, feature_theta, left_node_class, right_node_class)\n",
    "        f_ts.append((feature_idx, feature_theta, left_node_class, right_node_class))\n",
    "\n",
    "        # update weights\n",
    "        weights = [update_weight(train[w_idx, :], w, alpha, f_t) for w_idx, w in enumerate(weights)]\n",
    "        \n",
    "        # normalize weights\n",
    "        weight_sum = sum(weights)\n",
    "        weights = [float(x)/weight_sum for x in weights]\n",
    "        # print('sum of weights: ' + str(sum(weights)))\n",
    "        \n",
    "    parameters = collections.namedtuple('Parameters', ['alphas', 'functions'])\n",
    "    params = parameters(alphas, f_ts)\n",
    "    return params\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n",
      "192\n",
      "135\n",
      "67\n",
      "63\n",
      "46\n",
      "43\n",
      "39\n"
     ]
    }
   ],
   "source": [
    "trained_params = hw3_train_adaboost(_train, 1)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 2)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 4)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 8)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 16)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 32)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 64)\n",
    "print(hw3_test_adaboost(trained_params, _test))\n",
    "\n",
    "trained_params = hw3_train_adaboost(_train, 100)\n",
    "print(hw3_test_adaboost(trained_params, _test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss_of_data_set(train, test, num_round):\n",
    "    # num_rounds = [x for x in range(1, 101)]\n",
    "    loss = []\n",
    "    \n",
    "    trained_params = hw3_train_adaboost(train, num_round)\n",
    "    alphas = trained_params.alphas\n",
    "    f_ts = trained_params.functions\n",
    "    \n",
    "    \n",
    "    \n",
    "    for t in range(num_round):\n",
    "        \n",
    "        parameters = collections.namedtuple('Parameters', ['alphas', 'functions'])\n",
    "        params = parameters(alphas[0:t+1], f_ts[0:t+1])\n",
    "        l = hw3_test_adaboost(params, test)\n",
    "        loss.append(l)\n",
    "    \n",
    "    return loss\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[192, 192, 61, 135, 58, 64, 57, 67, 50, 65, 51, 65, 58, 56, 61, 63, 51, 63, 50, 61, 50, 57, 49, 49, 48, 45, 46, 48, 44, 47, 43, 46, 43, 44, 51, 43, 51, 43, 50, 47, 45, 48, 44, 47, 43, 48, 43, 47, 43, 44, 42, 46, 42, 45, 44, 44, 46, 44, 43, 44, 43, 44, 43, 43, 43, 43, 42, 42, 42, 41, 44, 41, 43, 42, 39, 43, 39, 43, 39, 43, 40, 43, 39, 42, 39, 41, 40, 40, 40, 40, 40, 40, 40, 39, 39, 40, 39, 39, 39, 39]\n"
     ]
    }
   ],
   "source": [
    "# training error\n",
    "# train_error = loss_of_data_set(_train, _train, 100)\n",
    "\n",
    "test_error = loss_of_data_set(_train, _test, 100)\n",
    "\n",
    "# print(train_error)\n",
    "print(test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[596, 596, 180, 434, 185, 211, 176, 213, 148, 202, 155, 187, 154, 172, 172, 190, 150, 181, 142, 176, 142, 167, 144, 145, 151, 138, 139, 151, 131, 149, 130, 142, 127, 130, 145, 129, 142, 125, 135, 133, 127, 135, 125, 130, 125, 132, 122, 133, 117, 127, 122, 131, 121, 129, 124, 121, 120, 124, 116, 123, 113, 120, 114, 112, 116, 119, 114, 117, 113, 108, 113, 104, 113, 109, 108, 111, 107, 109, 105, 107, 106, 107, 106, 108, 107, 108, 106, 110, 105, 107, 105, 107, 105, 106, 104, 108, 107, 110, 107, 108]\n"
     ]
    }
   ],
   "source": [
    "# training error\n",
    "train_error = loss_of_data_set(_train, _train, 100)\n",
    "print(train_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1.394    6.     159.       1.   ]\n",
      " [   1.567    6.     428.       0.   ]\n",
      " [   3.222    9.      29.       0.   ]\n",
      " [   2.815   26.     107.       1.   ]\n",
      " [   4.971   76.     517.       0.   ]]\n",
      "costs: \n",
      "[(0.20000000000000001, 159.0, 1, -1), (0.40000000000000002, 6.0, -1, -1), (0.20000000000000001, 1.3939999999999999, 1, -1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2, 159.0, 1, -1)"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_train[0:5, 54:]\n",
    "\n",
    "print(_train[0:5, 54:])\n",
    "find_dt(_train[0:5, 54:], np.ones(5)/5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "<h1>Plot for Part 2 here</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import spline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3601"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_test)\n",
    "len(_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "num_rounds = np.array([x for x in range(1,101)]) # x-axis\n",
    "loss = np.array([192, 192, 135, 67, 63, 46, 43, 39]) # y-axis\n",
    "\n",
    "#training data\n",
    "train_loss = np.array([596, 596, 434, 213, 190, 142, 112, 108])\n",
    "\n",
    "# xnew = np.linspace(num_rounds.min(), num_rounds.max(), 300)\n",
    "# loss_smooth = spline(num_rounds, loss, xnew)\n",
    "\n",
    "# plt.figure()\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(num_rounds, loss/1000, '-' , label='test')\n",
    "ax.plot(num_rounds, train_loss/3601, '--', label='train')\n",
    "ax.set_xlabel('num rounds')\n",
    "ax.set_ylabel('error rate')\n",
    "\n",
    "legend = ax.legend(loc='upper center', shadow=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(function, point):\n",
    "    '''\n",
    "    This class classifies the result of a point by using the given function, a weak function.\n",
    "    function: a 4 elements tuple, 1.feature_index, 2.theta value, 3.left node class, 4. right node class\n",
    "    returns one of the values of left or right.\n",
    "    '''\n",
    "    \n",
    "    f_idx = function[0]\n",
    "    f_theta = function[1]\n",
    "    left_node_class = function[2]\n",
    "    right_node_class = function[3]\n",
    "    \n",
    "    if point[f_idx] <= f_theta:\n",
    "        return left_node_class\n",
    "    else:\n",
    "        return right_node_class\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hw3_test_adaboost(params, test_data):\n",
    "    alphas = params.alphas\n",
    "    functions = params.functions\n",
    "    loss = 0\n",
    "    \n",
    "    for td in test_data:\n",
    "        # print(td)\n",
    "        \n",
    "        sum = 0\n",
    "        for idx, f in enumerate(functions):\n",
    "            # feature_idx, feature_theta, left_node_class, right_node_class\n",
    "            sum = sum + alphas[idx] * classify(f, td)\n",
    "        \n",
    "        if sum * td[-1] < 0:\n",
    "            loss = loss + 1\n",
    "            \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3, 5])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3,4])\n",
    "b = np.array([0,2,0,1])\n",
    "np.dot(a,b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 5],\n",
       "       [1, 6],\n",
       "       [2, 7],\n",
       "       [3, 8],\n",
       "       [4, 9]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(10).reshape(2,5).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, 1])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 3])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([[1,4,3], [3,1,2], [1,2,3]])\n",
    "z[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 4],\n",
       "       [1, 2, 3],\n",
       "       [1, 2, 3]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(z, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [2, 3, 4]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.vstack(([1,2,3],[2,3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = np.array([1,2,3,4,5])\n",
    "b = np.array([0,-1,0,-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = np.vstack((a,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4  5]\n",
      " [ 0 -1  0 -1  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(c)\n",
    "c[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.14\n",
      "0.78\n",
      "0.16\n",
      "0.32\n",
      "0.57\n",
      "0.56\n",
      "0.04\n",
      "0.07\n",
      "0.65\n",
      "0.09\n",
      "0.65\n",
      "0.6\n",
      "0.16\n",
      "0.51\n",
      "0.11\n",
      "0.17\n",
      "0.31\n",
      "0.58\n",
      "0.12\n",
      "0.19\n",
      "0.21\n",
      "0.09\n",
      "0.32\n",
      "0.46\n",
      "0.53\n",
      "0.63\n",
      "0.64\n",
      "0.65\n",
      "0.36\n",
      "0.15\n",
      "0.19\n",
      "0.62\n",
      "0.03\n",
      "0.47\n",
      "0.21\n",
      "0.17\n",
      "0.4\n",
      "0.17\n",
      "0.18\n",
      "0.04\n",
      "0.08\n",
      "0.19\n",
      "0.24\n",
      "0.17\n",
      "0.05\n",
      "0.32\n",
      "0.35\n",
      "0.1\n",
      "0.12\n",
      "0.53\n",
      "0.13\n",
      "0.19\n",
      "0.87\n",
      "0.36\n",
      "0.13\n",
      "0.64\n",
      "0.2\n",
      "0.19\n",
      "0.1\n",
      "0.2\n",
      "0.29\n",
      "0.6\n",
      "0.1\n",
      "0.05\n",
      "0.34\n",
      "0.2\n",
      "0.19\n",
      "0.8\n",
      "0.03\n",
      "0.05\n",
      "0.54\n",
      "0.67\n",
      "1.02\n",
      "1.26\n",
      "0.38\n",
      "0.11\n",
      "0.6\n",
      "0.44\n",
      "0.35\n",
      "0.23\n",
      "0.16\n",
      "0.46\n",
      "0.11\n",
      "0.1\n",
      "0.36\n",
      "0.32\n",
      "0.27\n",
      "0.44\n",
      "0.58\n",
      "0.09\n",
      "2.63\n",
      "0.09\n",
      "0.05\n",
      "0.12\n",
      "0.6\n",
      "0.45\n",
      "0.35\n",
      "0.45\n",
      "0.8\n",
      "0.21\n",
      "0.4\n",
      "0.73\n",
      "0.61\n",
      "0.08\n",
      "0.98\n",
      "0.1\n",
      "0.21\n",
      "0.38\n",
      "0.16\n",
      "0.44\n",
      "0.3\n",
      "0.61\n",
      "0.16\n",
      "0.25\n",
      "0.16\n",
      "0.61\n",
      "0.99\n",
      "0.44\n",
      "0.98\n",
      "0.21\n",
      "0.1\n",
      "0.22\n",
      "0.25\n",
      "0.58\n",
      "0.59\n",
      "0.47\n",
      "0.27\n",
      "0.21\n",
      "0.05\n",
      "0.26\n",
      "0.16\n",
      "1.09\n",
      "0.33\n",
      "0.22\n",
      "0.65\n",
      "0.9\n",
      "0.57\n",
      "0.19\n",
      "0.57\n",
      "0.57\n",
      "0.42\n",
      "0.13\n",
      "0.4\n",
      "0.17\n",
      "0.26\n",
      "0.05\n",
      "0.11\n",
      "0.12\n",
      "0.67\n",
      "0.09\n",
      "0.18\n",
      "0.03\n",
      "1.01\n",
      "1.02\n",
      "1.12\n",
      "0.23\n",
      "0.36\n",
      "0.77\n",
      "0.1\n",
      "0.1\n",
      "0.5\n",
      "0.33\n",
      "0.24\n",
      "0.17\n",
      "1.29\n",
      "0.29\n",
      "0.68\n",
      "0.25\n",
      "0.36\n",
      "0.1\n",
      "0.24\n",
      "0.03\n",
      "0.17\n",
      "0.32\n",
      "0.3\n",
      "0.8\n",
      "0.12\n",
      "0.27\n",
      "0.11\n",
      "0.22\n",
      "0.8\n",
      "0.26\n",
      "0.23\n",
      "0.5\n",
      "0.99\n",
      "1.02\n",
      "0.34\n",
      "0.55\n",
      "1.02\n",
      "0.22\n",
      "0.06\n",
      "0.13\n",
      "1.06\n",
      "0.84\n",
      "1.21\n",
      "0.65\n",
      "0.18\n",
      "1.27\n",
      "0.49\n",
      "0.49\n",
      "0.46\n",
      "0.58\n",
      "0.62\n",
      "0.37\n",
      "3.44\n",
      "0.56\n",
      "0.04\n",
      "0.09\n",
      "0.64\n",
      "0.61\n",
      "0.19\n",
      "0.07\n",
      "0.15\n",
      "0.43\n",
      "0.39\n",
      "0.19\n",
      "0.09\n",
      "0.28\n",
      "0.8\n",
      "0.16\n",
      "0.74\n",
      "1.28\n",
      "0.98\n",
      "0.64\n",
      "0.42\n",
      "0.21\n",
      "0.76\n",
      "0.14\n",
      "0.55\n",
      "0.27\n",
      "0.6\n",
      "0.63\n",
      "0.03\n",
      "0.44\n",
      "0.19\n",
      "0.09\n",
      "0.19\n",
      "0.11\n",
      "0.77\n",
      "1.1\n",
      "0.26\n",
      "0.29\n",
      "0.26\n",
      "0.8\n",
      "0.4\n",
      "0.25\n",
      "0.15\n",
      "0.74\n",
      "0.23\n",
      "0.37\n",
      "0.62\n",
      "0.46\n",
      "0.87\n",
      "0.56\n",
      "0.58\n",
      "0.27\n",
      "0.1\n",
      "1.0\n",
      "1.14\n",
      "0.27\n",
      "0.51\n",
      "0.49\n",
      "0.12\n",
      "0.45\n",
      "0.2\n",
      "0.1\n",
      "0.24\n",
      "0.08\n",
      "0.63\n",
      "0.38\n",
      "0.57\n",
      "0.49\n",
      "0.58\n",
      "0.17\n",
      "0.6\n",
      "0.24\n",
      "0.51\n",
      "0.17\n",
      "0.42\n",
      "0.09\n",
      "1.42\n",
      "0.36\n",
      "0.09\n",
      "0.23\n",
      "0.4\n",
      "0.29\n",
      "0.28\n",
      "0.37\n",
      "0.19\n",
      "0.42\n",
      "0.63\n",
      "1.05\n",
      "0.08\n",
      "0.29\n",
      "0.14\n",
      "0.13\n",
      "0.22\n",
      "0.39\n",
      "0.38\n",
      "1.29\n",
      "0.03\n",
      "0.1\n",
      "0.72\n",
      "0.59\n",
      "0.3\n",
      "0.23\n",
      "0.74\n",
      "1.32\n",
      "0.1\n",
      "0.19\n",
      "0.32\n",
      "0.22\n",
      "0.3\n",
      "0.64\n",
      "0.18\n",
      "1.27\n",
      "0.55\n",
      "0.29\n",
      "0.18\n",
      "0.48\n",
      "0.2\n",
      "1.32\n",
      "0.32\n",
      "1.61\n",
      "0.36\n",
      "0.67\n",
      "0.11\n",
      "0.25\n",
      "0.09\n",
      "0.09\n",
      "0.65\n",
      "0.27\n",
      "0.45\n",
      "0.98\n",
      "0.43\n",
      "0.08\n",
      "1.32\n",
      "0.11\n",
      "0.17\n",
      "0.3\n",
      "0.16\n",
      "2.1\n",
      "0.56\n",
      "0.08\n",
      "0.11\n",
      "0.33\n",
      "0.18\n",
      "0.6\n",
      "0.31\n",
      "0.8\n",
      "0.09\n",
      "0.18\n",
      "0.86\n",
      "0.1\n",
      "0.03\n",
      "0.98\n",
      "0.14\n",
      "0.39\n",
      "0.32\n",
      "0.22\n",
      "0.32\n",
      "0.32\n",
      "0.4\n",
      "0.1\n",
      "0.33\n",
      "0.17\n",
      "0.43\n",
      "0.37\n",
      "0.27\n",
      "0.67\n",
      "0.49\n",
      "0.65\n",
      "0.05\n",
      "0.1\n",
      "0.05\n",
      "0.1\n",
      "0.94\n",
      "1.03\n",
      "0.29\n",
      "0.55\n",
      "0.57\n",
      "0.1\n",
      "0.02\n",
      "0.55\n",
      "0.41\n",
      "0.2\n",
      "0.1\n",
      "0.76\n",
      "0.57\n",
      "0.94\n",
      "0.65\n",
      "0.07\n",
      "0.03\n",
      "0.17\n",
      "0.19\n",
      "0.27\n",
      "0.05\n",
      "0.34\n",
      "0.94\n",
      "0.1\n",
      "0.32\n",
      "0.32\n",
      "1.57\n",
      "0.03\n",
      "0.62\n",
      "0.09\n",
      "0.62\n",
      "0.28\n",
      "0.05\n",
      "0.09\n",
      "0.17\n",
      "0.08\n",
      "0.09\n",
      "1.05\n",
      "0.64\n",
      "0.47\n",
      "0.39\n",
      "0.37\n",
      "0.12\n",
      "0.38\n",
      "0.06\n",
      "0.14\n",
      "0.77\n",
      "0.1\n",
      "0.1\n",
      "0.12\n",
      "0.45\n",
      "0.74\n",
      "0.11\n",
      "0.05\n",
      "0.1\n",
      "0.23\n",
      "0.33\n",
      "0.26\n",
      "0.25\n",
      "0.44\n",
      "0.06\n",
      "0.17\n",
      "0.12\n",
      "0.29\n",
      "0.37\n",
      "0.73\n",
      "0.16\n",
      "0.23\n",
      "0.28\n",
      "0.49\n",
      "0.25\n",
      "0.2\n",
      "0.54\n",
      "1.88\n",
      "0.36\n",
      "0.16\n",
      "0.32\n",
      "1.17\n",
      "0.5\n",
      "0.32\n",
      "0.29\n",
      "0.61\n",
      "0.13\n",
      "0.55\n",
      "0.35\n",
      "0.78\n",
      "0.38\n",
      "0.93\n",
      "0.51\n",
      "0.44\n",
      "0.16\n",
      "0.6\n",
      "0.51\n",
      "2.94\n",
      "0.51\n",
      "0.74\n",
      "0.79\n",
      "0.22\n",
      "0.76\n",
      "0.18\n",
      "0.19\n",
      "0.03\n",
      "0.13\n",
      "0.49\n",
      "1.04\n",
      "1.12\n",
      "0.63\n",
      "0.98\n",
      "0.48\n",
      "1.88\n",
      "0.23\n",
      "0.57\n",
      "1.29\n",
      "0.53\n",
      "0.3\n",
      "0.31\n",
      "0.14\n",
      "0.08\n",
      "1.47\n",
      "0.05\n",
      "0.08\n",
      "0.23\n",
      "0.04\n",
      "0.11\n",
      "0.13\n",
      "0.51\n",
      "0.1\n",
      "0.44\n",
      "0.14\n",
      "0.33\n",
      "0.74\n",
      "0.43\n",
      "0.11\n",
      "0.34\n",
      "0.35\n",
      "0.41\n",
      "0.11\n",
      "0.36\n",
      "0.26\n",
      "0.38\n",
      "0.34\n",
      "0.32\n",
      "0.3\n",
      "0.08\n",
      "0.14\n",
      "0.16\n",
      "0.79\n",
      "0.62\n",
      "0.08\n",
      "0.1\n",
      "0.6\n",
      "0.2\n",
      "0.62\n",
      "0.48\n",
      "0.1\n",
      "0.09\n",
      "0.64\n",
      "0.89\n",
      "0.66\n",
      "0.25\n",
      "0.3\n",
      "0.66\n",
      "0.38\n",
      "0.43\n",
      "0.52\n",
      "0.03\n",
      "0.9\n",
      "0.2\n",
      "0.03\n",
      "0.15\n",
      "1.02\n",
      "0.1\n",
      "0.96\n",
      "0.65\n",
      "0.09\n",
      "1.64\n",
      "0.06\n",
      "0.11\n",
      "0.23\n",
      "0.47\n",
      "0.08\n",
      "0.11\n",
      "0.79\n",
      "0.73\n",
      "0.57\n",
      "0.31\n",
      "0.03\n",
      "0.77\n",
      "0.9\n",
      "0.25\n",
      "0.64\n",
      "0.24\n",
      "0.33\n",
      "0.23\n",
      "0.22\n",
      "0.28\n",
      "0.08\n",
      "0.32\n",
      "0.34\n",
      "0.56\n",
      "0.51\n",
      "0.16\n",
      "0.36\n",
      "0.3\n",
      "0.22\n",
      "0.35\n",
      "0.2\n",
      "0.65\n",
      "0.26\n",
      "0.08\n",
      "0.7\n",
      "0.47\n",
      "0.03\n",
      "1.4\n",
      "0.15\n",
      "0.25\n",
      "0.64\n",
      "0.09\n",
      "1.04\n",
      "0.18\n",
      "0.43\n",
      "0.05\n",
      "0.3\n",
      "0.09\n",
      "0.22\n",
      "0.34\n",
      "0.95\n",
      "0.32\n",
      "0.13\n",
      "0.16\n",
      "0.05\n",
      "0.22\n",
      "0.7\n",
      "0.57\n",
      "0.62\n",
      "0.55\n",
      "0.3\n",
      "0.28\n",
      "1.49\n",
      "2.3\n",
      "0.01\n",
      "0.09\n",
      "1.32\n",
      "1.28\n",
      "0.38\n",
      "0.63\n",
      "1.63\n",
      "0.6\n",
      "0.07\n",
      "0.09\n",
      "0.66\n",
      "0.19\n",
      "0.05\n",
      "0.34\n",
      "0.92\n",
      "1.1\n",
      "1.25\n",
      "0.78\n",
      "0.87\n",
      "0.25\n",
      "0.19\n",
      "0.24\n",
      "0.27\n",
      "0.65\n",
      "0.04\n",
      "0.08\n",
      "0.75\n",
      "0.14\n",
      "0.11\n",
      "1.1\n",
      "1.63\n",
      "0.55\n",
      "0.51\n",
      "0.16\n",
      "0.25\n",
      "0.44\n",
      "0.42\n",
      "0.78\n",
      "0.3\n",
      "0.08\n",
      "0.48\n",
      "0.62\n",
      "0.65\n",
      "5.88\n",
      "0.16\n",
      "1.02\n",
      "0.29\n",
      "0.32\n",
      "0.19\n",
      "0.26\n",
      "0.14\n",
      "0.49\n",
      "0.02\n",
      "1.2\n",
      "0.8\n",
      "0.25\n",
      "0.22\n",
      "0.08\n",
      "1.02\n",
      "0.54\n",
      "0.05\n",
      "0.12\n",
      "0.19\n",
      "0.74\n",
      "0.16\n",
      "0.09\n",
      "0.4\n",
      "0.43\n",
      "0.48\n",
      "0.29\n",
      "0.24\n",
      "0.71\n",
      "0.33\n",
      "0.91\n",
      "0.91\n",
      "0.36\n",
      "0.39\n",
      "0.03\n",
      "0.02\n",
      "0.13\n",
      "0.65\n",
      "0.26\n",
      "0.22\n",
      "0.47\n",
      "0.26\n",
      "0.13\n",
      "0.33\n",
      "0.11\n",
      "0.38\n",
      "0.09\n",
      "0.22\n",
      "0.8\n",
      "1.32\n",
      "0.13\n",
      "0.78\n",
      "0.11\n",
      "0.21\n",
      "0.66\n",
      "0.26\n",
      "0.31\n",
      "0.03\n",
      "0.36\n",
      "0.82\n",
      "0.15\n",
      "0.35\n",
      "0.8\n",
      "0.19\n",
      "0.13\n",
      "0.41\n",
      "0.13\n",
      "0.09\n",
      "0.26\n",
      "0.33\n",
      "1.02\n",
      "0.66\n",
      "0.39\n",
      "0.08\n",
      "0.59\n",
      "0.44\n",
      "0.45\n",
      "1.01\n",
      "0.54\n",
      "2.43\n",
      "1.29\n",
      "0.47\n",
      "0.44\n",
      "0.09\n",
      "0.29\n",
      "0.2\n",
      "0.22\n",
      "0.12\n",
      "0.48\n",
      "0.43\n",
      "0.17\n",
      "0.16\n",
      "0.46\n",
      "0.27\n",
      "2.1\n",
      "0.51\n",
      "0.73\n",
      "0.05\n",
      "0.8\n",
      "0.84\n",
      "1.11\n",
      "0.26\n",
      "1.29\n",
      "0.25\n",
      "0.07\n",
      "0.43\n",
      "1.01\n",
      "1.07\n",
      "0.65\n",
      "0.15\n",
      "1.34\n",
      "0.2\n",
      "0.48\n",
      "0.23\n",
      "0.73\n",
      "0.65\n",
      "1.02\n",
      "0.09\n",
      "0.57\n",
      "0.54\n"
     ]
    }
   ],
   "source": [
    "for x in _train[:, 5]:\n",
    "    if x != 0:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.00000000e+00,   5.70000000e-01,   5.70000000e-01,\n",
       "         0.00000000e+00,   1.40000000e-01,   1.40000000e-01,\n",
       "         0.00000000e+00,   0.00000000e+00,   1.40000000e-01,\n",
       "         0.00000000e+00,   0.00000000e+00,   4.30000000e-01,\n",
       "         1.40000000e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   1.40000000e-01,   0.00000000e+00,\n",
       "         3.31000000e+00,   0.00000000e+00,   1.44000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   5.70000000e-01,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.56000000e-01,   0.00000000e+00,   0.00000000e+00,\n",
       "         1.39400000e+00,   6.00000000e+00,   1.59000000e+02,\n",
       "         1.00000000e+00])"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn.tree\n",
    "len(_train[0])\n",
    "_train[0, 0:58]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h = sklearn.tree.DecisionTreeClassifier(max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  1.  0.  1.  0.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1.,  0.,  0.,  1.,  0.,  1.,  0.,  0.,  1.,  0.])"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws = np.ones(10)/10\n",
    "h.fit( _train[0:10,0:57], _train[0:10, 57], sample_weight=ws )\n",
    "pred = h.predict(_train[0:10, 0:57])\n",
    "print(train[0:10, 57])\n",
    "pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 -1 -1]\n",
      "[ 2 -1 -1]\n",
      "[51 -2 -2]\n",
      "[ 0.0385 -2.     -2.    ]\n"
     ]
    }
   ],
   "source": [
    "print(h.tree_.children_left)\n",
    "print(h.tree_.children_right)\n",
    "print(h.tree_.feature)\n",
    "print(h.tree_.threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
